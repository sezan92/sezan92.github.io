# Reinforcement Learning Course 1 Week 4

> Notes for Reinforcement Learning Course 1 Week 4.

## TLDR

- Evaluate the policy: How good the policy is?
- Update the policy: Let's improve our actions!
- Dynamic Programming in the context of Reinforcement learning

### Prerequisites

Previous blog , [Reinforcement Learning Course 1, Week 3](https://sezan92.github.io/2023/11/21/RL-course1-w3-blog.html)

## Policy Evaluation

From the previous blog, we came to know about value of a state. We can denote this as $v(s)$. Now the value of a state will depend on two things. One, how easily we can get the maximum reward from the state. Two, is what action we will take, i.e. what policy we will follow on that state! We can denote it as $v_{\pi}(s)$. Which means, the "value of a state while following the policy $\pi$".

![](/images/RL_1_W4_blog/image_1_Policy_Evaluation.png)

From the first blog, we can recall

$v_{\pi} (s) = \displaystyle\sum_a \pi(a|s) \displaystyle\sum_{s'}\displaystyle\sum_r p(s', r|s, a)[r + \gamma v_{\pi} (s')]$


For each state we will get different equation right? From the equations we should be able to solve and get the optimum value $v_{\pi}$! But the problem is that in practice, the states are in many cases continuous! That means infinite states and hence, infinite equations are possible! Not to mention for different policies $\pi$, the equations will change! So what do we do? We go for iterative solution! We use Dynamic Programming algorithms (yeah, that dynamic programming which is used in problem solving!)

![](/images/RL_1_W4_blog/image_2_LinearSolver_DP.png)